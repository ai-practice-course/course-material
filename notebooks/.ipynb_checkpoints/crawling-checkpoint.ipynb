{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "light-leone",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'webob'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5fb4f5454871>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# from pykrx import stock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwebob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murlparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'webob'"
     ]
    }
   ],
   "source": [
    "import json, logging, os, re, requests, sys, time\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.request import urlopen, Request\n",
    "from datetime import date, datetime\n",
    "# from pykrx import stock\n",
    "from webob.compat import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "referenced-accused",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaverFinanceNewsCrawler:\n",
    "    def __init__(self):\n",
    "        self.ticker = None\n",
    "        self.result = []\n",
    "        pass\n",
    "\n",
    "    def _crawl_by_query(self, ticker, dt):\n",
    "        \"\"\"\n",
    "        Crawl Naver Finance News\n",
    "        :param ticker: string; search keywords\n",
    "        :return: generator; [{title, summary, url, articleId, content, codes}, ...]\n",
    "        \"\"\"\n",
    "        comp_name = stock.get_market_ticker_name(ticker)\n",
    "        print(\"comp name = \", comp_name)\n",
    "        # Convert the query to euc-kr string\n",
    "        q = \"\"\n",
    "        for c in comp_name.encode(\"euc-kr\"):\n",
    "            q += \"%%%s\" % format(c, \"x\").capitalize()\n",
    "\n",
    "        # 여러 페이지를 동시에 돌려주므로, 기사가 존재할 때까지 loop 돌면서 뉴스 가져오기\n",
    "        page = 1\n",
    "        n_news = 0\n",
    "\n",
    "        while True:\n",
    "            r_url = URL_NAVER_FINANCE_NEWS_QUERY % (q, dt, dt, page,)\n",
    "            r = requests.get(r_url)\n",
    "            soup = bs(r.text, \"lxml\")\n",
    "            news = soup.find(\"div\", class_=\"newsSchResult\").find(\n",
    "                \"dl\", class_=\"newsList\"\n",
    "            )\n",
    "            news_title = news.find_all(\"dd\", class_=\"articleSubject\")\n",
    "            news_summary = news.find_all(\"dd\", class_=\"articleSummary\")\n",
    "            wdate = news.find_all(\"span\", class_=\"wdate\")\n",
    "            n_news += len(news_title)\n",
    "\n",
    "            if len(news_title) > 0:\n",
    "                for title, summary, _date in zip(news_title, news_summary, wdate):\n",
    "                    date = _date.find(text=True).split(\"\\n\")[2].strip()\n",
    "\n",
    "                    if date == dt:\n",
    "                        url = URL_NAVER_FINANCE + title.a.get(\"href\")\n",
    "                        res = {\n",
    "                            \"title\": title.a.text,\n",
    "                            \"summary\": summary.find(text=True).strip(\" \\t\\n\\r\"),\n",
    "                            \"url\": url,\n",
    "                            \"ticker\": ticker,\n",
    "                            \"articleId\": urlparse.parse_qs(\n",
    "                                urlparse.urlparse(url).query\n",
    "                            )[\"article_id\"][0],\n",
    "                            \"date\": date,\n",
    "                        }\n",
    "                        res.update(self._crawl_content(url))\n",
    "                        #             if self.query in res['title']:\n",
    "                        self.result.append(res)\n",
    "                        insert_doc('naver_news', res)\n",
    "                        time.sleep(1)\n",
    "                page += 1\n",
    "\n",
    "            else:\n",
    "                break\n",
    "        print(\"number of news for comp {} = {}\".format(comp_name, n_news))\n",
    "\n",
    "    def _crawl_content(self, url):\n",
    "        r = requests.get(url)\n",
    "        soup = bs(r.text, \"lxml\")\n",
    "        content = soup.find(\"div\", id=\"content\", class_=\"articleCont\")\n",
    "        codes = re.findall(r\"\\d{6}\", content.text)\n",
    "        cntnt = content.text.strip(\" \\t\\n\\r\").split(\"@\")[0]\n",
    "        # 마지막 마침표 이후 제거\n",
    "        effix = cntnt.split('.')[-1]\n",
    "        cntnt = cntnt.replace(effix, '')\n",
    "        return {\"content\": cntnt}\n",
    "\n",
    "    def get_valid_ticker_name(self, ticker):\n",
    "        try:\n",
    "            return stock.get_market_ticker_name(ticker)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def get_news(self, ticker, dt):\n",
    "        # logger instance 생성\n",
    "        logger = logging.getLogger(__name__)\n",
    "        # Format 설정\n",
    "        formatter = logging.Formatter(\n",
    "            \"%(asctime)s %(levelname)s [%(name)s] [%(filename)s:%(lineno)d] - %(message)s\"\n",
    "        )\n",
    "        # handler 생성\n",
    "        streamHandler = logging.StreamHandler()\n",
    "        streamHandler.setFormatter(formatter)\n",
    "        logger.addHandler(streamHandler)\n",
    "        logger.setLevel(level=logging.INFO)\n",
    "\n",
    "        today = date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        if dt is None:\n",
    "            dt = today\n",
    "        else:\n",
    "            dt = dt\n",
    "\n",
    "        # 회사 정보 만들기.\n",
    "        _comp = {}\n",
    "        _comp[\"ticker\"] = ticker\n",
    "        _comp[\"name\"] = [self.get_valid_ticker_name(x) for x in ticker]\n",
    "\n",
    "        # 이미 있는 날짜와 회사 정보 가져오기\n",
    "        docs = get_all_docs('naver_news', {\"query\": {\"match_all\": {}}})\n",
    "        already_exists = set(\n",
    "            [x['_source'][\"ticker\"] + x['_source'][\"date\"] for x in get_all_docs('naver_news', {\"query\": {\"match_all\": {}}})]\n",
    "        )\n",
    "\n",
    "        for i, t in enumerate(_comp[\"ticker\"]):\n",
    "            if len(already_exists) > 0:\n",
    "                exist = t + dt in already_exists and True or False\n",
    "            else:\n",
    "                exist = False\n",
    "\n",
    "            print('i = {} , comp name = {} , ticker = {}'.format(i, _comp[\"name\"][i], _comp[\"ticker\"][i]))\n",
    "            if _comp[\"name\"][i] is not None and not exist:\n",
    "                logger.info(\"i = {}, comp = {}\".format(i, _comp[\"ticker\"][i]))\n",
    "                try:\n",
    "                    msg = \"company = {}\".format(_comp[\"name\"][i])\n",
    "                    logger.info(msg)\n",
    "                    time.sleep(1)\n",
    "                    self._crawl_by_query(ticker=t, dt=dt)\n",
    "                except Exception as e:\n",
    "                    logger.error(e)\n",
    "                    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "allied-underground",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_news() missing 1 required positional argument: 'dt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-69e0502315f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mNNC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNaverFinanceNewsCrawler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mNNC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_news\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'005930'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'20220322'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%Y%m%d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: get_news() missing 1 required positional argument: 'dt'"
     ]
    }
   ],
   "source": [
    "NNC = NaverFinanceNewsCrawler\n",
    "NNC.get_news('005930', datetime.strptime('20220322', '%Y%m%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "related-correction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2022, 3, 22, 0, 0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " datetime.strptime('20220322', '%Y%m%d')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-baltimore",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

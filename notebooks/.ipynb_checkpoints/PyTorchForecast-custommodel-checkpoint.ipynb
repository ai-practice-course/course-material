{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "statistical-weather",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "global-franklin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from pytorch_forecasting.models.base_model import BaseModelWithCovariates\n",
    "from pytorch_forecasting.models.nn import MultiEmbedding\n",
    "from pytorch_forecasting import TimeSeriesDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caroline-bangkok",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FullyConnectedModule(nn.Module):\n",
    "    def __init__(self, input_size: int, output_size: int, hidden_size: int, n_hidden_layers: int):\n",
    "        super().__init__()\n",
    "        module_list = [\n",
    "            nn.Linear(input_size, hidden_size), nn.ReLU()\n",
    "        ]\n",
    "        for _ in range(n_hidden_layers):\n",
    "            module_list.extend([\n",
    "                nn.Linear(hidden_size, hidden_size), nn.ReLU()\n",
    "            ])\n",
    "        module_list.append(nn.Linear(hidden_size, output_size))\n",
    "        self.sequential = nn.Sequential(*module_list)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x of shape: batch_size * n_timesteps_in\n",
    "        # output of shape batch_size * n_timesteps_out\n",
    "        return self.sequential(x)\n",
    "    \n",
    "network = FullyConnectedModule(input_size = 5, output_size = 2, hidden_size = 10, n_hidden_layers = 2)\n",
    "x = torch.rand(20, 5)\n",
    "network(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "distant-coaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedModel(BaseModelWithCovariates):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size: int, \n",
    "        output_size: int, \n",
    "        hidden_size: int, \n",
    "        n_hidden_layers: int, \n",
    "        x_reals: List[str],\n",
    "        x_categoricals: List[str],\n",
    "        embedding_sizes: Dict[str, Tuple[int, int]],\n",
    "        embedding_labels: Dict[str, List[str]],\n",
    "        static_categoricals: List[str],\n",
    "        static_reals: List[str],\n",
    "        time_varying_categoricals_encoder: List[str],\n",
    "        time_varying_categoricals_decoder: List[str],\n",
    "        time_varying_reals_encoder: List[str],\n",
    "        time_varying_reals_decoder: List[str],\n",
    "        embedding_paddings: List[str],\n",
    "        categorical_groups: Dict[str, List[str]],\n",
    "        **kwargs):\n",
    "        # saves arguments in signature to '.hparams' attribute, mandatory call\n",
    "        self.save_hyperparameters()\n",
    "        # pass additional arguments to BaseModel.__init__, mandatory call\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # create embedder\n",
    "        self.input_embeddings = MultiEmbedding(\n",
    "            embedding_sizes = self.hparams.embedding_sizes,\n",
    "            categorical_groups = self.hparams.categorical_groups,\n",
    "            embedding_paddings = self.hparams.embedding_paddings,\n",
    "            x_categoricals = self.hparams.x_categoricals,\n",
    "            max_embedding_size = self.hparams.hidden_size,\n",
    "        )\n",
    "        \n",
    "        # Calculate the size of all concatenated embeddings + continuous variables\n",
    "        n_features = sum(\n",
    "            embedding_size for classes_size, embedding_size in self.hparams.embedding_sizes.values()\n",
    "        ) + len(self.reals)\n",
    "        \n",
    "        self.network = FullyConnectedModule(\n",
    "            input_size = self.hparams.input_size * n_features,\n",
    "            output_size = self.hparams.output_size,\n",
    "            hidden_size = self.hparams.hidden_size,\n",
    "            n_hidden_layers = self.hparams.n_hidden_layers,\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        # x is a batch generated based on the TimeSeriesDataset\n",
    "        batch_size = x['encoder_lengths'].size(0)\n",
    "        embeddings = self.input_embeddings(x['encoder_cat']) # returns dictionary with embedding tensors\n",
    "        network_input = torch.cat(\n",
    "            [x['encoder_cont']] + \n",
    "                [\n",
    "                    emb for name, emb in embeddings.items()\n",
    "                    if name in self.encoder_variables or name in self.static_variables\n",
    "                ], dim = -1,\n",
    "        )\n",
    "        prediction = self.network(network_input.view(batch_size, -1))\n",
    "        \n",
    "        prediction = self.transform_output(prediction, target_scale = x['target_scale'])\n",
    "        \n",
    "        # We need to return a dictionary that at least contains the prediction\n",
    "        # The parameter can be directly forwarded from the input.\n",
    "        # The conversion to a named tuple can be directly achieved with the `to_network_output` function.\n",
    "        return self.to_network_output(prediction=prediction)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataset(cls, dataset: TimeSeriesDataSet, **kwargs):\n",
    "        new_kwargs = {\n",
    "            'output_size': dataset.max_prediction_length,\n",
    "            'input_size': dataset.max_encoder_length,\n",
    "        }\n",
    "        new_kwargs.update(kwargs)\n",
    "        assert dataset.max_prediction_length == dataset.min_prediction_length, \"Decoder only supports a fixed length\"\n",
    "        assert dataset.min_encoder_length == dataset.max_encoder_length, \"Encoder only supports a fixed length\"\n",
    "        return super().from_dataset(dataset, **new_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-preference",
   "metadata": {},
   "source": [
    "## Data 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "trying-princess",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/crypto_currency.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-benchmark",
   "metadata": {},
   "source": [
    "#### Create index\n",
    "  * 모형에서 integer ndex를 timestamp로 하고 있음\n",
    "  * 가장 긴 데이터를 기준으로 index 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "approved-fighter",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "m = 0\n",
    "for k in data.keys():\n",
    "    cnt += data[k].shape[0]\n",
    "    m = np.max([m, data[k].shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "corporate-edition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(720, 16)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['KRW-ZIL'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ethical-meter",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = data['KRW-ZIL']\n",
    "d1 = d1.sort_values('candle_date_time_kst')\n",
    "d1['index'] = np.arange(len(d1))\n",
    "date_index = d1[['index', 'candle_date_time_kst']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "large-photography",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103848\n"
     ]
    }
   ],
   "source": [
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "front-threat",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_data = pd.DataFrame()\n",
    "for i, key in enumerate(data.keys()):\n",
    "    tmp = data[key]\n",
    "    tmp_1 = pd.merge(tmp, date_index, on ='candle_date_time_kst')\n",
    "    try:\n",
    "        res_data = pd.concat([res_data, tmp_1[['market', 'candle_acc_trade_volume', 'index', 'change_price', 'candle_date_time_kst', 'trade_price']]])\n",
    "    except Exception as e:\n",
    "        res_data = pd.concat([res_data, tmp_1[['market', 'candle_acc_trade_volume', 'index_y', 'change_price', 'candle_date_time_kst', 'trade_price']]])\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-folder",
   "metadata": {},
   "source": [
    "### Dupe 발생\n",
    "* Dupe 나는 data 파악"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "czech-audio",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_data['new_idx'] = res_data['market'] + res_data['index'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cardiac-taylor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new_idx</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BTC-AERGO517</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BTC-AERGO518</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BTC-AERGO519</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BTC-AHT519</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BTC-ANKR518</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BTC-ANKR519</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BTC-ARDR519</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BTC-CBK519</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BTC-DKA519</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BTC-FCT2519</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BTC-HUM519</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BTC-IQ519</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BTC-LOOM519</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BTC-RFR517</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BTC-RFR518</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BTC-RFR519</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BTC-XEM519</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BTC-ZIL519</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USDT-SC519</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              index\n",
       "new_idx            \n",
       "BTC-AERGO517      2\n",
       "BTC-AERGO518      2\n",
       "BTC-AERGO519      2\n",
       "BTC-AHT519        2\n",
       "BTC-ANKR518       2\n",
       "BTC-ANKR519       2\n",
       "BTC-ARDR519       2\n",
       "BTC-CBK519        2\n",
       "BTC-DKA519        2\n",
       "BTC-FCT2519       2\n",
       "BTC-HUM519        2\n",
       "BTC-IQ519         2\n",
       "BTC-LOOM519       2\n",
       "BTC-RFR517        2\n",
       "BTC-RFR518        2\n",
       "BTC-RFR519        2\n",
       "BTC-XEM519        2\n",
       "BTC-ZIL519        2\n",
       "USDT-SC519        2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_data.groupby(['new_idx']).count()[['index']][res_data.groupby(['new_idx']).count()['index'] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "acceptable-panel",
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_new_idx = res_data.groupby(['new_idx']).count()[['index']][res_data.groupby(['new_idx']).count()['index'] > 1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "binding-cemetery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['BTC-AERGO517', 'BTC-AERGO518', 'BTC-AERGO519', 'BTC-AHT519',\n",
       "       'BTC-ANKR518', 'BTC-ANKR519', 'BTC-ARDR519', 'BTC-CBK519', 'BTC-DKA519',\n",
       "       'BTC-FCT2519', 'BTC-HUM519', 'BTC-IQ519', 'BTC-LOOM519', 'BTC-RFR517',\n",
       "       'BTC-RFR518', 'BTC-RFR519', 'BTC-XEM519', 'BTC-ZIL519', 'USDT-SC519'],\n",
       "      dtype='object', name='new_idx')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dup_new_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-jenny",
   "metadata": {},
   "source": [
    "### Dupe 나는 row 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "gothic-measure",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_data.drop_duplicates(subset = ['new_idx'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-intervention",
   "metadata": {},
   "source": [
    "### Error 발생 -> AssertionError: data index has to be unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "overall-advertising",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_data.set_index('new_idx', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ethical-football",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TimeSeriesDataSet(\n",
    "    res_data[['market', 'trade_price', 'index', 'candle_acc_trade_volume']], # , 'change_price'에는 미싱값이 많음\n",
    "    group_ids = ['market'],\n",
    "    target = 'trade_price',\n",
    "    time_idx = 'index',\n",
    "    min_encoder_length = 5,\n",
    "    max_encoder_length = 5,\n",
    "    min_prediction_length = 2,\n",
    "    max_prediction_length = 2,\n",
    "    time_varying_unknown_reals = ['trade_price'],\n",
    "    time_varying_known_reals = ['candle_acc_trade_volume'],\n",
    "    time_varying_known_categoricals = [],\n",
    "    static_categoricals = ['market'],   \n",
    "    allow_missing_timesteps=True, #미싱값이 있는 경우\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "requested-barcelona",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'time_idx': 'index',\n",
       " 'target': 'trade_price',\n",
       " 'group_ids': ['market'],\n",
       " 'weight': None,\n",
       " 'max_encoder_length': 5,\n",
       " 'min_encoder_length': 5,\n",
       " 'min_prediction_idx': 0,\n",
       " 'min_prediction_length': 2,\n",
       " 'max_prediction_length': 2,\n",
       " 'static_categoricals': ['market'],\n",
       " 'static_reals': [],\n",
       " 'time_varying_known_categoricals': [],\n",
       " 'time_varying_known_reals': ['candle_acc_trade_volume'],\n",
       " 'time_varying_unknown_categoricals': [],\n",
       " 'time_varying_unknown_reals': ['trade_price'],\n",
       " 'variable_groups': {},\n",
       " 'constant_fill_strategy': {},\n",
       " 'allow_missing_timesteps': True,\n",
       " 'lags': {},\n",
       " 'add_relative_time_idx': False,\n",
       " 'add_target_scales': False,\n",
       " 'add_encoder_length': False,\n",
       " 'target_normalizer': GroupNormalizer(transformation='log'),\n",
       " 'categorical_encoders': {'__group_id__market': NaNLabelEncoder(),\n",
       "  'market': NaNLabelEncoder()},\n",
       " 'scalers': {'candle_acc_trade_volume': StandardScaler()},\n",
       " 'randomize_length': None,\n",
       " 'predict_mode': False}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.get_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "built-beginning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  {'encoder_cat': tensor([[[112],\n",
      "         [112],\n",
      "         [112],\n",
      "         [112],\n",
      "         [112]],\n",
      "\n",
      "        [[ 97],\n",
      "         [ 97],\n",
      "         [ 97],\n",
      "         [ 97],\n",
      "         [ 97]],\n",
      "\n",
      "        [[154],\n",
      "         [154],\n",
      "         [154],\n",
      "         [154],\n",
      "         [154]],\n",
      "\n",
      "        [[118],\n",
      "         [118],\n",
      "         [118],\n",
      "         [118],\n",
      "         [118]]]), 'encoder_cont': tensor([[[-0.0376,  1.0103],\n",
      "         [-0.0376,  1.0105],\n",
      "         [-0.0376,  1.0099],\n",
      "         [-0.0376,  1.0101],\n",
      "         [-0.0376,  1.0027]],\n",
      "\n",
      "        [[-0.0376,  0.7061],\n",
      "         [-0.0376,  0.7015],\n",
      "         [-0.0376,  0.7048],\n",
      "         [-0.0376,  0.7034],\n",
      "         [-0.0376,  0.6982]],\n",
      "\n",
      "        [[-0.0376,  0.7011],\n",
      "         [-0.0376,  0.7011],\n",
      "         [-0.0376,  0.7039],\n",
      "         [-0.0376,  0.6863],\n",
      "         [-0.0376,  0.6885]],\n",
      "\n",
      "        [[-0.0376,  0.5295],\n",
      "         [-0.0376,  0.5295],\n",
      "         [-0.0376,  0.5323],\n",
      "         [-0.0376,  0.5351],\n",
      "         [-0.0376,  0.5404]]]), 'encoder_target': tensor([[4125.0000, 4135.0000, 4110.0000, 4120.0000, 3840.0000],\n",
      "        [ 235.0000,  225.0000,  232.0000,  229.0000,  218.0000],\n",
      "        [ 224.0000,  224.0000,  230.0000,  195.0000,  199.0000],\n",
      "        [  44.5000,   44.5000,   45.7000,   46.9000,   49.3000]]), 'encoder_lengths': tensor([5, 5, 5, 5]), 'decoder_cat': tensor([[[112],\n",
      "         [112]],\n",
      "\n",
      "        [[ 97],\n",
      "         [ 97]],\n",
      "\n",
      "        [[154],\n",
      "         [154]],\n",
      "\n",
      "        [[118],\n",
      "         [118]]]), 'decoder_cont': tensor([[[-0.0376,  0.9993],\n",
      "         [-0.0376,  0.9999]],\n",
      "\n",
      "        [[-0.0375,  0.7006],\n",
      "         [-0.0376,  0.6991]],\n",
      "\n",
      "        [[-0.0376,  0.6836],\n",
      "         [-0.0376,  0.6742]],\n",
      "\n",
      "        [[-0.0376,  0.5382],\n",
      "         [-0.0376,  0.5448]]]), 'decoder_target': tensor([[3720.0000, 3740.0000],\n",
      "        [ 223.0000,  220.0000],\n",
      "        [ 190.0000,  174.0000],\n",
      "        [  48.3000,   51.4000]]), 'decoder_lengths': tensor([2, 2, 2, 2]), 'decoder_time_idx': tensor([[658, 659],\n",
      "        [584, 585],\n",
      "        [ 49,  50],\n",
      "        [245, 246]]), 'groups': tensor([[112],\n",
      "        [ 97],\n",
      "        [154],\n",
      "        [118]]), 'target_scale': tensor([[-1.1933,  9.4215],\n",
      "        [-1.1933,  9.4215],\n",
      "        [-1.1933,  9.4215],\n",
      "        [-1.1933,  9.4215]])}\n",
      "\n",
      "y =  (tensor([[3720.0000, 3740.0000],\n",
      "        [ 223.0000,  220.0000],\n",
      "        [ 190.0000,  174.0000],\n",
      "        [  48.3000,   51.4000]]), None)\n",
      "\tencoder_cat = torch.Size([4, 5, 1])\n",
      "\tencoder_cont = torch.Size([4, 5, 2])\n",
      "\tencoder_target = torch.Size([4, 5])\n",
      "\tencoder_lengths = torch.Size([4])\n",
      "\tdecoder_cat = torch.Size([4, 2, 1])\n",
      "\tdecoder_cont = torch.Size([4, 2, 2])\n",
      "\tdecoder_target = torch.Size([4, 2])\n",
      "\tdecoder_lengths = torch.Size([4])\n",
      "\tdecoder_time_idx = torch.Size([4, 2])\n",
      "\tgroups = torch.Size([4, 1])\n",
      "\ttarget_scale = torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "dataloader = dataset.to_dataloader(batch_size = 4)\n",
    "x, y = next(iter(dataloader))\n",
    "print('x = ', x)\n",
    "print('\\ny = ', y)\n",
    "for key, value in x.items():\n",
    "    print(f'\\t{key} = {value.size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "subjective-louisville",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "#probabilities = np.sqrt(1 + data.loc[dataset.index, 'trade_price'])\n",
    "#sampler = WeightedRandomSampler(probabilities, len(probabilities))\n",
    "# dataset.to_dataloader(train = True,  shuffle = True) # sampler = sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "outdoor-commons",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   | Name                               | Type                 | Params\n",
       "-----------------------------------------------------------------------------\n",
       "0  | loss                               | SMAPE                | 0     \n",
       "1  | logging_metrics                    | ModuleList           | 0     \n",
       "2  | input_embeddings                   | MultiEmbedding       | 2.0 K \n",
       "3  | input_embeddings.embeddings        | ModuleDict           | 2.0 K \n",
       "4  | input_embeddings.embeddings.market | Embedding            | 2.0 K \n",
       "5  | network                            | FullyConnectedModule | 1.9 K \n",
       "6  | network.sequential                 | Sequential           | 1.9 K \n",
       "7  | network.sequential.0               | Linear               | 1.7 K \n",
       "8  | network.sequential.1               | ReLU                 | 0     \n",
       "9  | network.sequential.2               | Linear               | 110   \n",
       "10 | network.sequential.3               | ReLU                 | 0     \n",
       "11 | network.sequential.4               | Linear               | 110   \n",
       "12 | network.sequential.5               | ReLU                 | 0     \n",
       "13 | network.sequential.6               | Linear               | 22    \n",
       "-----------------------------------------------------------------------------\n",
       "3.9 K     Trainable params\n",
       "0         Non-trainable params\n",
       "3.9 K     Total params\n",
       "0.015     Total estimated model params size (MB)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FullyConnectedModel.from_dataset(dataset, input_size = 5, output_size = 2, hidden_size = 10, n_hidden_layers = 2)\n",
    "model.summarize(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "disabled-protocol",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (4x60 and 165x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-1b759c8a749f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.8.3/envs/image_crawler/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-cf3e987d7c76>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 ], dim = -1,\n\u001b[1;32m     57\u001b[0m         )\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target_scale'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.3/envs/image_crawler/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-0652ebd51779>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# x of shape: batch_size * n_timesteps_in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# output of shape batch_size * n_timesteps_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFullyConnectedModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_hidden_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.3/envs/image_crawler/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.3/envs/image_crawler/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.3/envs/image_crawler/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.3/envs/image_crawler/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.3/envs/image_crawler/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (4x60 and 165x10)"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(dataloader))\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "higher-regular",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>market</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>564</td>\n",
       "      <td>BTC-REI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>430</td>\n",
       "      <td>KRW-SRM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>538</td>\n",
       "      <td>BTC-AERGO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>634</td>\n",
       "      <td>KRW-AAVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index     market\n",
       "0    564    BTC-REI\n",
       "1    430    KRW-SRM\n",
       "2    538  BTC-AERGO\n",
       "3    634   KRW-AAVE"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.x_to_index(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "involved-manhattan",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting.models.nn import MultiEmbedding\n",
    "\n",
    "class FullyConnectedModel(BaseModelWithCovariates):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size: int, \n",
    "        output_size: int, \n",
    "        hidden_size: int, \n",
    "        n_hidden_layers: int, \n",
    "        x_reals: List[str],\n",
    "        x_categoricals: List[str],\n",
    "        embedding_sizes: Dict[str, Tuple[int, int]],\n",
    "        embedding_labels: Dict[str, List[str]],\n",
    "        static_categoricals: List[str],\n",
    "        static_reals: List[str],\n",
    "        time_varying_categoricals_encoder: List[str],\n",
    "        time_varying_categoricals_decoder: List[str],\n",
    "        time_varying_reals_encoder: List[str],\n",
    "        time_varying_reals_decoder: List[str],\n",
    "        embedding_paddings: List[str],\n",
    "        categorical_groups: Dict[str, List[str]],\n",
    "        **kwargs):\n",
    "        # saves arguments in signature to '.hparams' attribute, mandatory call\n",
    "        self.save_hyperparameters()\n",
    "        # pass additional arguments to BaseModel.__init__, mandatory call\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # create embedder\n",
    "        self.input_embeddings = MultiEmbedding(\n",
    "            embedding_sizes = self.hparams.embedding_sizes,\n",
    "            categorical_groups = self.hparams.categorical_groups,\n",
    "            embedding_padding = self.hparams.embedding_paddings,\n",
    "            x_categoricals = self.hparams.x_categoricals,\n",
    "            max_embedding_size = self.hparams.hidden_size,\n",
    "        )\n",
    "        \n",
    "        # Calculate the size of all concatenated embeddings + continuous variables\n",
    "        n_features = sum(\n",
    "            embedding_size for classes_size, embedding_size in self.hparams.embedding_sizes.values()\n",
    "        ) + len(self.reals)\n",
    "        \n",
    "        self.network = FullyConnectedModule(\n",
    "            input_size = self.hparams.input_size * n_features,\n",
    "            output_size = self.hparams.output_size,\n",
    "            hidden_size = self.hparams.hidden_size,\n",
    "            n_hidden_layers = self.hparams.n_hidden_layers,\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        # x is a batch generated based on the TimeSeriesDataset\n",
    "        batch_size = x['encoder_lengths'].size(0)\n",
    "        embeddings = self.input_embeddings(x['encoder_cat']) # returns dictionary with embedding tensors\n",
    "        network_input = torch.cat(\n",
    "            [x['encoder_cont']] + \n",
    "                [\n",
    "                    emb for name, emb in embeddings.items()\n",
    "                    if name in self.encoder_variables or name in self.static_variables\n",
    "                ], dim = -1,\n",
    "        )\n",
    "        prediction = self.network(network_input.view(batch_size, -1))\n",
    "        \n",
    "        prediction = self.transform_output(prediction, target_scale = x['target_scale'])\n",
    "        \n",
    "        # We need to return a dictionary that at least contains the prediction\n",
    "        # The parameter can be directly forwarded from the input.\n",
    "        # The conversion to a named tuple can be directly achieved with the `to_network_output` function.\n",
    "        return self.to_network_output(prediction=prediction)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataset(cls, dataset: TimeSeriesDataSet, **kwargs):\n",
    "        new_kwargs = {\n",
    "            'output_size': dataset.max_prediction_length,\n",
    "            'input_size': dataset.max_encoder_length,\n",
    "        }\n",
    "        new_kwargs.update(kwargs)\n",
    "        assert dataset.max_prediction_length == dataset.min_prediction_length, \"Decoder only supports a fixed length\"\n",
    "        assert dataset.min_encoder_length == dataset.max_encoder_length, \"Encoder only supports a fixed length\"\n",
    "        return super().from_dataset(dataset, **new_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "egyptian-uniform",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'embedding_padding'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-e98409a13058>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFullyConnectedModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_hidden_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-0c6e4a2bce2e>\u001b[0m in \u001b[0;36mfrom_dataset\u001b[0;34m(cls, dataset, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_prediction_length\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_prediction_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Decoder only supports a fixed length\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_encoder_length\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_encoder_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Encoder only supports a fixed length\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.8.3/envs/image_crawler/lib/python3.8/site-packages/pytorch_forecasting/models/base_model.py\u001b[0m in \u001b[0;36mfrom_dataset\u001b[0;34m(cls, dataset, allowed_encoder_known_variable_names, **kwargs)\u001b[0m\n\u001b[1;32m   1472\u001b[0m         )\n\u001b[1;32m   1473\u001b[0m         \u001b[0mnew_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1474\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1476\u001b[0m     def extract_features(\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.3/envs/image_crawler/lib/python3.8/site-packages/pytorch_forecasting/models/base_model.py\u001b[0m in \u001b[0;36mfrom_dataset\u001b[0;34m(cls, dataset, **kwargs)\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"output_transformer\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"output_transformer\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_normalizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 987\u001b[0;31m         \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    988\u001b[0m         \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_target\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-0c6e4a2bce2e>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_size, output_size, hidden_size, n_hidden_layers, x_reals, x_categoricals, embedding_sizes, embedding_labels, static_categoricals, static_reals, time_varying_categoricals_encoder, time_varying_categoricals_decoder, time_varying_reals_encoder, time_varying_reals_decoder, embedding_paddings, categorical_groups, **kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# create embedder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         self.input_embeddings = MultiEmbedding(\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0membedding_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_sizes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mcategorical_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical_groups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'embedding_padding'"
     ]
    }
   ],
   "source": [
    "model = FullyConnectedModel.from_dataset(dataset, hidden_size = 10, n_hidden_layers = 2)\n",
    "print(model.summarize(3))\n",
    "model.hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "innocent-rugby",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /home/kyle/course-material/notebooks/lightning_logs\n",
      "\n",
      "  | Name            | Type                 | Params\n",
      "---------------------------------------------------------\n",
      "0 | loss            | SMAPE                | 0     \n",
      "1 | logging_metrics | ModuleList           | 0     \n",
      "2 | network         | FullyConnectedModule | 302   \n",
      "---------------------------------------------------------\n",
      "302       Trainable params\n",
      "0         Non-trainable params\n",
      "302       Total params\n",
      "0.001     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "720acaf9c2fa4853b2c28e96e1d6a1a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "trainer = Trainer(max_epochs = 20, gpus = 0)\n",
    "trainer.fit(model, train_dataloaders = dataloader, val_dataloaders = dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-guitar",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

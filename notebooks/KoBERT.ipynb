{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "piano-acoustic",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "gentle-preview",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
      "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-ppi45o_h\n",
      "  Running command git clone --quiet 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-ppi45o_h\n",
      "  Resolved https://****@github.com/SKTBrain/KoBERT.git to commit e1f2f37055e7460d8427f6912579c0162cb69831\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting boto3\n",
      "  Downloading boto3-1.23.0-py3-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.5/132.5 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: gluonnlp>=0.6.0 in /home/kyle/.pyenv/versions/3.8.3/envs/image_crawler/lib/python3.8/site-packages (from kobert==0.2.3) (0.10.0)\n",
      "Requirement already satisfied: mxnet>=1.4.0 in /home/kyle/.pyenv/versions/3.8.3/envs/image_crawler/lib/python3.8/site-packages (from kobert==0.2.3) (1.6.0)\n",
      "Collecting onnxruntime==1.8.0\n",
      "  Downloading onnxruntime-1.8.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting sentencepiece>=0.1.6\n",
      "  Downloading sentencepiece-0.1.96-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.7.0 in /home/kyle/.pyenv/versions/3.8.3/envs/image_crawler/lib/python3.8/site-packages (from kobert==0.2.3) (1.10.2)\n",
      "Requirement already satisfied: transformers>=4.8.1 in /home/kyle/.pyenv/versions/3.8.3/envs/image_crawler/lib/python3.8/site-packages (from kobert==0.2.3) (4.16.2)\n",
      "Requirement already satisfied: protobuf in /home/kyle/.pyenv/versions/3.8.3/envs/image_crawler/lib/python3.8/site-packages (from onnxruntime==1.8.0->kobert==0.2.3) (3.19.4)\n",
      "Requirement already satisfied: flatbuffers in /home/kyle/.pyenv/versions/3.8.3/envs/image_crawler/lib/python3.8/site-packages (from onnxruntime==1.8.0->kobert==0.2.3) (2.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /home/kyle/.pyenv/versions/3.8.3/envs/image_crawler/lib/python3.8/site-packages (from onnxruntime==1.8.0->kobert==0.2.3) (1.21.6)\n",
      "Requirement already satisfied: packaging in /home/kyle/.pyenv/versions/3.8.3/envs/image_crawler/lib/python3.8/site-packages (from gluonnlp>=0.6.0->kobert==0.2.3) (21.3)\n",
      "Requirement already satisfied: cython in /home/kyle/.pyenv/versions/3.8.3/envs/image_crawler/lib/python3.8/site-packages (from gluonnlp>=0.6.0->kobert==0.2.3) (0.29.28)\n",
      "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /home/kyle/.pyenv/versions/3.8.3/envs/image_crawler/lib/python3.8/site-packages (from mxnet>=1.4.0->kobert==0.2.3) (0.8.4)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /home/kyle/.pyenv/versions/3.8.3/envs/image_crawler/lib/python3.8/site-packages (from mxnet>=1.4.0->kobert==0.2.3) (2.25.1)\n",
      "Requirement already satisfied: typing-extensions in /home/kyle/.pyenv/versions/3.8.3/envs/image_crawler/lib/python3.8/site-packages (from torch>=1.7.0->kobert==0.2.3) (4.0.1)\n",
      "Requirement already satisfied: filelock in /home/kyle/.pyenv/versions/3.8.3/envs/image_crawler/lib/python3.8/site-packages (from transformers>=4.8.1->kobert==0.2.3) (3.6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/kyle/.pyenv/versions/3.8.3/envs/image_crawler/lib/python3.8/site-packages (from transformers>=4.8.1->kobert==0.2.3) (2022.1.18)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/kyle/.pyenv/versions/3.8.3/envs/image_crawler/lib/python3.8/site-packages (from transformers>=4.8.1->kobert==0.2.3) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /home/kyle/.pyenv/versions/3.8.3/envs/image_crawler/lib/python3.8/site-packages (from transformers>=4.8.1->kobert==0.2.3) (0.11.5)\n",
      "Requirement already satisfied: sacremoses in /home/kyle/.pyenv/versions/3.8.3/envs/image_crawler/lib/python3.8/site-packages (from transformers>=4.8.1->kobert==0.2.3) (0.0.47)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/kyle/.pyenv/versions/3.8.3/envs/image_crawler/lib/python3.8/site-packages (from transformers>=4.8.1->kobert==0.2.3) (4.62.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/kyle/.pyenv/versions/3.8.3/envs/image_crawler/lib/python3.8/site-packages (from transformers>=4.8.1->kobert==0.2.3) (0.4.0)\n",
      "Collecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)\n",
      "Collecting s3transfer<0.6.0,>=0.5.0\n",
      "  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 KB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting botocore<1.27.0,>=1.26.0\n",
      "  Downloading botocore-1.26.0-py3-none-any.whl (8.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.25.4 in /home/kyle/.pyenv/versions/3.8.3/envs/image_crawler/lib/python3.8/site-packages (from botocore<1.27.0,>=1.26.0->boto3->kobert==0.2.3) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/kyle/.pyenv/versions/3.8.3/envs/image_crawler/lib/python3.8/site-packages (from botocore<1.27.0,>=1.26.0->boto3->kobert==0.2.3) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/kyle/.pyenv/versions/3.8.3/envs/image_crawler/lib/python3.8/site-packages (from packaging->gluonnlp>=0.6.0->kobert==0.2.3) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/kyle/.pyenv/versions/3.8.3/envs/image_crawler/lib/python3.8/site-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.2.3) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/kyle/.pyenv/versions/3.8.3/envs/image_crawler/lib/python3.8/site-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.2.3) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kyle/.pyenv/versions/3.8.3/envs/image_crawler/lib/python3.8/site-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->kobert==0.2.3) (2020.12.5)\n",
      "Requirement already satisfied: six in /home/kyle/.pyenv/versions/3.8.3/envs/image_crawler/lib/python3.8/site-packages (from sacremoses->transformers>=4.8.1->kobert==0.2.3) (1.15.0)\n",
      "Requirement already satisfied: joblib in /home/kyle/.pyenv/versions/3.8.3/envs/image_crawler/lib/python3.8/site-packages (from sacremoses->transformers>=4.8.1->kobert==0.2.3) (1.1.0)\n",
      "Requirement already satisfied: click in /home/kyle/.pyenv/versions/3.8.3/envs/image_crawler/lib/python3.8/site-packages (from sacremoses->transformers>=4.8.1->kobert==0.2.3) (8.0.4)\n",
      "Building wheels for collected packages: kobert\n",
      "  Building wheel for kobert (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kobert: filename=kobert-0.2.3-py3-none-any.whl size=15663 sha256=70c36378aed2664c88f5a91fd3eb03e2a25351fe6a1145f254e36a6cf08ad3a1\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-opiiw0zt/wheels/bf/5f/74/81bf3a1332130eb6629ecf58876a8746b77021e7d7b0638e91\n",
      "Successfully built kobert\n",
      "Installing collected packages: sentencepiece, onnxruntime, jmespath, botocore, s3transfer, boto3, kobert\n",
      "Successfully installed boto3-1.23.0 botocore-1.26.0 jmespath-1.0.0 kobert-0.2.3 onnxruntime-1.8.0 s3transfer-0.5.2 sentencepiece-0.1.96\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/home/kyle/.pyenv/versions/image_crawler/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adolescent-efficiency",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "\n",
    "\n",
    "##GPU 사용 시\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "qualified-point",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kyle/course-material/notebooks/.cache/kobert_v1.zip[██████████████████████████████████████████████████]\n",
      "/home/kyle/course-material/notebooks/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece[██████████████████████████████████████████████████]\n"
     ]
    }
   ],
   "source": [
    "bertmodel, vocab = get_pytorch_kobert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "municipal-coach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /home/kyle/course-material/notebooks/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
    "# Tokenizer 불러오기\n",
    "# seq_len을 정해야 한다\n",
    "trans = nlp.data.BERTSentenceTransform(tok, max_seq_length=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "agreed-father",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-05-14 08:55:07--  https://www.dropbox.com/s/374ftkec978br3d/ratings_train.txt?dl=1\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.84.18, 2620:100:6034:18::a27d:5412\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.84.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: /s/dl/374ftkec978br3d/ratings_train.txt [following]\n",
      "--2022-05-14 08:55:07--  https://www.dropbox.com/s/dl/374ftkec978br3d/ratings_train.txt\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uc5b5579c936a0bb4fd4d802e2fe.dl.dropboxusercontent.com/cd/0/get/BlOV7VDRgoUt_fxkl_JGmh6mq-6ZENo76DQ9pchCHxSVm4Yay6qarsJUzeyjwtjcEbZKaxeXvfXHxEiL0YG4UCBELRlNiHkAj3qF66a8vYO9XyLRYO76kdXsWvg-OKsOAFqJRrszXRTnoT73vSk2V2gCUghw75Pg9VqRrV3W5ZoVyw/file?dl=1# [following]\n",
      "--2022-05-14 08:55:07--  https://uc5b5579c936a0bb4fd4d802e2fe.dl.dropboxusercontent.com/cd/0/get/BlOV7VDRgoUt_fxkl_JGmh6mq-6ZENo76DQ9pchCHxSVm4Yay6qarsJUzeyjwtjcEbZKaxeXvfXHxEiL0YG4UCBELRlNiHkAj3qF66a8vYO9XyLRYO76kdXsWvg-OKsOAFqJRrszXRTnoT73vSk2V2gCUghw75Pg9VqRrV3W5ZoVyw/file?dl=1\n",
      "Resolving uc5b5579c936a0bb4fd4d802e2fe.dl.dropboxusercontent.com (uc5b5579c936a0bb4fd4d802e2fe.dl.dropboxusercontent.com)... 162.125.84.15, 2620:100:6034:15::a27d:540f\n",
      "Connecting to uc5b5579c936a0bb4fd4d802e2fe.dl.dropboxusercontent.com (uc5b5579c936a0bb4fd4d802e2fe.dl.dropboxusercontent.com)|162.125.84.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 14628807 (14M) [application/binary]\n",
      "Saving to: ‘ratings_train.txt?dl=1’\n",
      "\n",
      "ratings_train.txt?d 100%[===================>]  13.95M  18.6MB/s    in 0.7s    \n",
      "\n",
      "2022-05-14 08:55:09 (18.6 MB/s) - ‘ratings_train.txt?dl=1’ saved [14628807/14628807]\n",
      "\n",
      "--2022-05-14 08:55:09--  https://www.dropbox.com/s/977gbwh542gdy94/ratings_test.txt?dl=1\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.84.18, 2620:100:6034:18::a27d:5412\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.84.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: /s/dl/977gbwh542gdy94/ratings_test.txt [following]\n",
      "--2022-05-14 08:55:10--  https://www.dropbox.com/s/dl/977gbwh542gdy94/ratings_test.txt\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uc13867c737c2ee47f05517ba557.dl.dropboxusercontent.com/cd/0/get/BlNQ2R29_KDeTCIAEr5Sbua6Ho4M0ZxeLngKseJbMuaCwLg53TET2nEdrkaW54rVLRPutqoIg5d6PxfaCNkjKHkAM7Ifu8nal7VNOMYovNgXt-wuJnzNuxn02Xo95F8d1qelvnfxEfclw0OPloJzfEdFhfw5SpIMOS359BkSNIdp8w/file?dl=1# [following]\n",
      "--2022-05-14 08:55:10--  https://uc13867c737c2ee47f05517ba557.dl.dropboxusercontent.com/cd/0/get/BlNQ2R29_KDeTCIAEr5Sbua6Ho4M0ZxeLngKseJbMuaCwLg53TET2nEdrkaW54rVLRPutqoIg5d6PxfaCNkjKHkAM7Ifu8nal7VNOMYovNgXt-wuJnzNuxn02Xo95F8d1qelvnfxEfclw0OPloJzfEdFhfw5SpIMOS359BkSNIdp8w/file?dl=1\n",
      "Resolving uc13867c737c2ee47f05517ba557.dl.dropboxusercontent.com (uc13867c737c2ee47f05517ba557.dl.dropboxusercontent.com)... 162.125.84.15, 2620:100:6034:15::a27d:540f\n",
      "Connecting to uc13867c737c2ee47f05517ba557.dl.dropboxusercontent.com (uc13867c737c2ee47f05517ba557.dl.dropboxusercontent.com)|162.125.84.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4893335 (4.7M) [application/binary]\n",
      "Saving to: ‘ratings_test.txt?dl=1’\n",
      "\n",
      "ratings_test.txt?dl 100%[===================>]   4.67M  12.7MB/s    in 0.4s    \n",
      "\n",
      "2022-05-14 08:55:11 (12.7 MB/s) - ‘ratings_test.txt?dl=1’ saved [4893335/4893335]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.dropbox.com/s/374ftkec978br3d/ratings_train.txt?dl=1\n",
    "!wget https://www.dropbox.com/s/977gbwh542gdy94/ratings_test.txt?dl=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "level-mouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, vocab, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, vocab=vocab, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "classical-apple",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters\n",
    "max_len = 64\n",
    "batch_size = 64\n",
    "\n",
    "dataset_test = nlp.data.TSVDataset(\"ratings_test.txt?dl=1\", field_indices=[1,2], num_discard_samples=1)\n",
    "data_test = BERTDataset(dataset_test, 0, 1, tok, vocab, max_len, True, False)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "breeding-pixel",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=2,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.softmax(self.classifier(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "large-italy",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "minor-copying",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "romance-wildlife",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-ec03b3c9e46c>:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a8af841b4bd4baeaeda6f04273580b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output = tensor([[0.4693, 0.5307],\n",
      "        [0.3634, 0.6366],\n",
      "        [0.6216, 0.3784]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Test acc 0.5010190217391305\n"
     ]
    }
   ],
   "source": [
    "test_acc = 0\n",
    "for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "    token_ids = token_ids.long().to(device)\n",
    "    segment_ids = segment_ids.long().to(device)\n",
    "    valid_length= valid_length\n",
    "    label = label.long().to(device)\n",
    "    out = model(token_ids, valid_length, segment_ids)\n",
    "    test_acc += calc_accuracy(out, label)\n",
    "    if batch_id == 0:\n",
    "      print(f'output = {out[:3]}')\n",
    "print(\"Test acc {}\".format(test_acc / (batch_id+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-offense",
   "metadata": {},
   "source": [
    "## Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "suited-czech",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from transformers.optimization import  get_cosine_schedule_with_warmup\n",
    "\n",
    "# Parameters\n",
    "\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 5\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "rough-myanmar",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = nlp.data.TSVDataset(\"ratings_train.txt?dl=1\", field_indices=[1,2], num_discard_samples=1)\n",
    "data_train = BERTDataset(dataset_train, 0, 1, tok, vocab, max_len, True, False)\n",
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "equal-antique",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "copyrighted-advancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "lyric-rochester",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-34-e6a38b13095b>:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d06b2489399b4fc6adfa3565135946bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 0.7098344564437866 train acc 0.5\n",
      "epoch 1 batch id 201 loss 0.5460638999938965 train acc 0.5666977611940298\n",
      "epoch 1 batch id 401 loss 0.48434382677078247 train acc 0.6744856608478803\n",
      "epoch 1 batch id 601 loss 0.4649675488471985 train acc 0.7239236688851913\n",
      "epoch 1 batch id 801 loss 0.5512837171554565 train acc 0.7518921660424469\n",
      "epoch 1 batch id 1001 loss 0.42420637607574463 train acc 0.7658123126873126\n",
      "epoch 1 batch id 1201 loss 0.49051886796951294 train acc 0.7781015820149875\n",
      "epoch 1 batch id 1401 loss 0.5262107849121094 train acc 0.7855884189864383\n",
      "epoch 1 batch id 1601 loss 0.44049566984176636 train acc 0.7925320112429731\n",
      "epoch 1 batch id 1801 loss 0.44042542576789856 train acc 0.7983932537479178\n",
      "epoch 1 batch id 2001 loss 0.438401997089386 train acc 0.8039183533233384\n",
      "epoch 1 batch id 2201 loss 0.477384090423584 train acc 0.8082121762835075\n",
      "epoch 1 train acc 0.8112867960750854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-34-e6a38b13095b>:22: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cdf2902442d4d3a810f2d15c376d27b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 test acc 0.8600343670076727\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af43fd0568df46a8979f2dbdeb3e2b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 0.4896039068698883 train acc 0.828125\n",
      "epoch 2 batch id 201 loss 0.4185541570186615 train acc 0.8530783582089553\n",
      "epoch 2 batch id 401 loss 0.4374520778656006 train acc 0.8576215710723192\n",
      "epoch 2 batch id 601 loss 0.4948432743549347 train acc 0.860700915141431\n",
      "epoch 2 batch id 801 loss 0.5192050933837891 train acc 0.862105961298377\n",
      "epoch 2 batch id 1001 loss 0.4511871635913849 train acc 0.8642763486513486\n",
      "epoch 2 batch id 1201 loss 0.41489556431770325 train acc 0.8659450457951707\n",
      "epoch 2 batch id 1401 loss 0.4346323311328888 train acc 0.8660666488222698\n",
      "epoch 2 batch id 1601 loss 0.47707274556159973 train acc 0.8671435821361649\n",
      "epoch 2 batch id 2001 loss 0.45090818405151367 train acc 0.8690342328835582\n",
      "epoch 2 batch id 2201 loss 0.42931118607521057 train acc 0.8702791344843253\n",
      "epoch 2 train acc 0.8716514682878271\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f19ab6e7775449eba7f9ca50fe8e7991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 test acc 0.8761988491048593\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a159a4d1e554929846b0e2dff2994cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1 loss 0.48991942405700684 train acc 0.796875\n",
      "epoch 3 batch id 201 loss 0.3644742965698242 train acc 0.8857276119402985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1201 loss 0.3669576644897461 train acc 0.8933831182348043\n",
      "epoch 3 batch id 1401 loss 0.3866939842700958 train acc 0.8933239650249821\n",
      "epoch 3 batch id 1601 loss 0.3914433717727661 train acc 0.8940603529044348\n",
      "epoch 3 batch id 1801 loss 0.41588982939720154 train acc 0.8954053303720155\n",
      "epoch 3 batch id 2001 loss 0.4139482378959656 train acc 0.8960207396301849\n",
      "epoch 3 batch id 2201 loss 0.3899887800216675 train acc 0.8965385052248978\n",
      "epoch 3 train acc 0.8975731477531286\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c8569716be14dad929848f7b9a6216b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 test acc 0.8864490089514067\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67f1251fc4364f34b7dbc2e5be42e58e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1 loss 0.4850531816482544 train acc 0.828125\n",
      "epoch 4 batch id 201 loss 0.37437599897384644 train acc 0.9067164179104478\n",
      "epoch 4 batch id 401 loss 0.3566151261329651 train acc 0.9098347880299252\n",
      "epoch 4 batch id 601 loss 0.43556728959083557 train acc 0.9124116056572379\n",
      "epoch 4 batch id 801 loss 0.42909952998161316 train acc 0.9142478152309613\n",
      "epoch 4 batch id 1001 loss 0.4396789073944092 train acc 0.9150068681318682\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-e6a38b13095b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/image_crawler/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/image_crawler/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-category",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ImageProcesss",
   "language": "python",
   "name": "image_crawler"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
